<HTML>
<BODY>
<p>
<font size=+2><b>Parallel performance of CM1:</b></font>
<p>
This page presents information about the performance of CM1 on distributed memory supercomputers.  
<p>
   <font size=+1>Contents:</font> | <a href="#strong">A Strong Scaling Test</a> | <a href="#weak">A Weak Scaling Test</a> |
<p>
<a name="strong"><hr></a>
<p>
<b><font color="blue" size=+1>A Strong Scaling Test with cm1r19 on NSF NCAR's <i>cheyenne</i></font></b>:  Supercell thunderstorm simulation  (posted 27 June 2017)
<p>
<b>System</b>:  NSF NCAR's <a href="https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne">cheyenne</a>:   SGI ICE XA Cluster with Intel Broadwell processors.  <br>
<b>Compiler</b>:  ifort 16.0.3<br>
<b>Code</b>:  cm1r19.1<br>
<b>CM1 Configuration</b>:  MPI<br>
<b>Case</b>:  Idealized supercell thunderstorm, 2 h integration, 250 m horizontal grid spacing  <br>
<b>Total domain dimensions</b>:  576 &times 576 &times 128  <br>
<b>Time steps</b>:  2,880  <br>
<b>NOTE</b>:  This is a <u><b>STRONG SCALING</b></u> test (i.e., problem size is fixed) that includes moisture as well as Input/Output<br>
<p>
<b><font color="red">Results with I/O:</font></b> (full 3d output every 15 min;  8 output times total;  28 GB total)<br> 
<br>
<img src="ch_r19.png">
<br>
<p>
<font color="blue">Comments</font>:  This test demonstrates that the CM1 solver scales reasonably well for ~10,000 cores (black line above).  However, when using more than 1,000 cores, the time required to write output begins to affect parallel performance.  For binary GrADS-format output (output_format = 1) (red line) parallel performance degrades beyond roughly 4,000 cores.   For netcdf output ("output_format = 2) (blue line) parallel performance does not scale well beyound roughly 1,000 cores. 
<p>
Results will vary depending on the frequency of output, the total size of output, and (especially) based on model physical schemes.  For example, simulations with more expensive microphysics schemes and simulations with atmospheric radiation will require more CPU time to complete. 
<p>
<font color="blue">Recommendation</font>:  For this configuration (that is, with the Morrison microphysics scheme, the LES subgrid turbulence model, and no radiation scheme), a good formula for estimating core-hours required for a simulation on NSF NCAR's cheyenne supercomputer is:  
<br>
<img src="Eqn1.png">
<br>
where <i>C</i> is the total number of cheyenne core-hours, <i>N<sub>x</sub></i> is the number of grid points in the <i>x</i> direction, <i>N<sub>y</sub></i> is the number of grid points in the <i>y</i> direction, <i>N<sub>z</sub></i> is the number of grid points in the <i>z</i> direction, and <i>N<sub>t</sub></i> is the total number of timesteps.  
<p>
For example:   a 512 &times 512 &times 128 domain, integrated for 2 hours with a 2.5-s timestep (and thus 2,880 total time steps), would require approximately 242 cheyenne core-hours.   So, assuming 144 cores (i.e., 4 nodes) are used, then roughly 1.7 wallclock hours would be needed to run this simulation on cheyenne.
<p>
Also:  for large simulations that require large processor counts, users of CM1 should use binary GrADS-format output (output_format = 1) to ensure acceptable parallel performance.  If netcdf-format output is required, then conversion software can be used after the CM1 simulation is complete.  For example, the Climate Data Operators (CDO) package can be used to convert GrADS-format data to netcdf format.  For example, on cheyenne, type: 
 <pre>module load nco</pre> 
 followed by a command of form: 
<pre>cdo -f nc4 import_binary cm1out_s.ctl cm1out_s.nc</pre> 
<p>
<p>
<a name="weak"><hr></a>
<p>
<b><font color="blue" size=+1>A Weak Scaling Test with cm1r19 on NSF NCAR's <i>cheyenne</i></font></b>:  Large Eddy Simulation of a convective boundary layer  (posted 29 June 2017)
<p>
<b>System</b>:  NSF NCAR's <a href="https://www2.cisl.ucar.edu/resources/computational-systems/cheyenne">cheyenne</a>:   SGI ICE XA Cluster with Intel Broadwell processors.  <br>
<b>Compiler</b>:  ifort 16.0.3<br>
<b>Code</b>:  cm1r19.1<br>
<b>CM1 Configuration</b>:  MPI<br>
<b>Case</b>:  LES of convective boundary layer, 40 m horizontal grid spacing  <br>
<b>Domain dimensions</b>:  varies with number of cores (i.e., processors).  Each core has 16 &times 16 &times 128 grid points.  The largest total domain size is 3,072 &times 3,072 &times 128 grid points. <br>
<b>Time steps</b>:  3,600  <br>
<b>NOTE</b>:  This is a <u><b>WEAK SCALING</b></u> test (i.e., problem size scales with number of processors) that does not include moisture/microphysics but <i>does</i> include Input/Output<br>
<p>
<b><font color="red">Results with I/O:</font></b> (full 3d output every 15 min;  9 output times total;  18 MB per core)<br> 
<br>
<img src="ch_r19_weak.png">
<br>
<p>
<font color="blue">Comments</font>:   For this test the domain size increases as cores are added, with the goal of testing how CM1 performs for very large domains.  In this case, the total run time should (ideally) remain the same as cores are added. 
<p>
Results in the figure above show that the CM1 solver performs well to at least 36,000 cores (black line).  For more that roughly 10,000 cores, however, the time required to write output begins to degrade parallel performance.  For binary GrADS-format output (red line) parallel performance is not seriously impacted until roughly 20,000 cores.   For the netcdf output (blue line), however, parallel performance is negatively impacted with only a few thousand cores. 
<p>
<font color="blue">Recommendation</font>:  For large datasets that require large processor counts, users of CM1 should use binary GrADS-format output (output_format = 1) to ensure acceptable parallel performance.  If netcdf-format output is required, then conversion software can be used after the CM1 simulation is complete.  For example, the Climate Data Operators (CDO) package can be used to convert GrADS-format data to netcdf format.  For example, on cheyenne, type: 
 <pre>module load nco</pre> 
 followed by a command of form: 
<pre>cdo -f nc4 import_binary cm1out_s.ctl cm1out_s.nc</pre> 
<p>
<hr>
<p>
<i>Last updated:  29 June 2017</i>
<p>
<a href="https://www2.mmm.ucar.edu/people/bryan/cm1/">return to cm1 home page</a>
<p>
<hr>
<p>
The National Center for Atmospheric Research is sponsored by the U.S. National Science Foundation. Any opinions, findings and conclusions or recommendations expressed in this publication are those of the author(s) and do not necessarily reflect the views of the U.S. National Science Foundation.
<p>
<p>
</BODY>
</HTML>
